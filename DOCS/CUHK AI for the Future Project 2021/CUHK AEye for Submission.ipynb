{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhicPbNdT0Y7"
   },
   "source": [
    "# 1. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7jkLfr3T0ZI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib djitellopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCmltTkCT0ZS"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from djitellopy import tello\n",
    "from time import sleep \n",
    "from scipy import stats\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    if image is not None:\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "      image.flags.writeable = False                  \n",
    "      results = model.process(image)                 \n",
    "      image.flags.writeable = True                   \n",
    "      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "      return image, results\n",
    "    \n",
    "    else:\n",
    "      print(\"empty frame\")\n",
    "    \n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "def extract_keypoints_holistic(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]) \n",
    "\n",
    "def extract_face_keypoints(results):\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    return face \n",
    "\n",
    "actions_holistic = np.array(['grpA','grpB','grpC','grpD','grpE'])\n",
    "no_sequences_holistic = 15\n",
    "sequence_length_holistic = 30\n",
    "\n",
    "actions_face = np.array(['normal-face', 'elongated'])\n",
    "no_sequences_face = 15\n",
    "sequence_length_face = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn1gbFsTT0bK"
   },
   "source": [
    "# 3. LSTM Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b9KyXVUT0bL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "model_hol = Sequential() \n",
    "model_hol.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model_hol.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model_hol.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model_hol.add(Dense(64, activation='relu')) \n",
    "model_hol.add(Dense(32, activation='relu'))\n",
    "model_hol.add(Dense(actions_holistic.shape[0], activation='softmax')) \n",
    "\n",
    "model_hol.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the LSTM_hol.h5 file and update the path. \n",
    "model_hol.load_weights(r\"C:\\Users\\tangk\\LSTM_hol.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qY-3tp-TT0bY",
    "outputId": "855168cd-4466-4fee-a716-802b8a328bcb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_hol.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b9KyXVUT0bL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "model_f = Sequential() \n",
    "model_f.add(LSTM(32, return_sequences=True, activation='relu', input_shape=(30,1404)))\n",
    "model_f.add(LSTM(32, return_sequences=False, activation='relu'))\n",
    "model_f.add(Dense(32, activation='relu'))\n",
    "model_f.add(Dense(actions_face.shape[0], activation='softmax')) \n",
    "\n",
    "model_f.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['binary_accuracy']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the LSTM_face.h5 file and update the path. \n",
    "model_f.load_weights(r\"C:\\Users\\tangk\\LSTM_face.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qY-3tp-TT0bY",
    "outputId": "855168cd-4466-4fee-a716-802b8a328bcb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_f.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDbjLMluT0ce"
   },
   "source": [
    "# 5. Test in Real Time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: [__face-status, word-group__]\n",
    "## Supposed to be tested with masks on. The relative postions of gesture to your body matters. \n",
    "## refer to PPT for the gestures of group A, B, C, D and E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO 1: Prototype capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTRUCTIONS: Turn on the Tello. Connect to Tello wifi. Run the cell below (may require multiple runs) until the battery is displayed and the light turns green. Short-press to turn off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "me = tello.Tello()\n",
    "me.connect()\n",
    "print(me.get_battery())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the following cell to stream from Tello. Press 'q' to exit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence_hol = []\n",
    "sequence_f = []\n",
    "#sentence = []\n",
    "predictions_hol = []\n",
    "predictions_f = []\n",
    "present = []\n",
    "\n",
    "me.connect()\n",
    "me.streamon()\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while True: \n",
    "\n",
    "        frame = me.get_frame_read().frame\n",
    "        frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        \n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        keypoints_hol = extract_keypoints_holistic(results)\n",
    "        keypoints_f = extract_face_keypoints(results)\n",
    "        \n",
    "        sequence_hol.append(keypoints_hol)\n",
    "        sequence_f.append(keypoints_f)\n",
    "        \n",
    "        sequence_hol = sequence_hol[-30:]\n",
    "        sequence_f = sequence_f[-30:]\n",
    "        \n",
    "        \n",
    "        if len(sequence_hol) == 30:\n",
    "            res_hol = model_hol.predict(np.expand_dims(sequence_hol, axis=0))[0]\n",
    "            print(actions_holistic[np.argmax(res_hol)])\n",
    "            predictions_hol = np.argmax(res_hol)\n",
    "            \n",
    "            res_f = model_f.predict(np.expand_dims(sequence_f, axis=0))[0]\n",
    "            print(actions_face[np.argmax(res_f)])\n",
    "            predictions_f = np.argmax(res_f)\n",
    "            elongation = str(actions_face[np.argmax(res_f)])\n",
    "            present = str(elongation +' '+ actions_holistic[np.argmax(res_hol)])\n",
    "            \n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(present), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            \n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break       \n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO 2: Webcam capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FQjyCHTT0ck",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence_hol = []\n",
    "sequence_f = []\n",
    "predictions_hol = []\n",
    "predictions_f = []\n",
    "present = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints_hol = extract_keypoints_holistic(results)\n",
    "        keypoints_f = extract_face_keypoints(results)\n",
    "        \n",
    "        sequence_hol.append(keypoints_hol)\n",
    "        sequence_f.append(keypoints_f)\n",
    "        \n",
    "        sequence_hol = sequence_hol[-30:]\n",
    "        sequence_f = sequence_f[-30:]\n",
    "        \n",
    "        \n",
    "        if len(sequence_hol) == 30:\n",
    "            res_hol = model_hol.predict(np.expand_dims(sequence_hol, axis=0))[0]\n",
    "            print(actions_holistic[np.argmax(res_hol)])\n",
    "            predictions_hol = np.argmax(res_hol)\n",
    "            \n",
    "            res_f = model_f.predict(np.expand_dims(sequence_f, axis=0))[0]\n",
    "            print(actions_face[np.argmax(res_f)])\n",
    "            predictions_f = np.argmax(res_f)\n",
    "            elongation = str(actions_face[np.argmax(res_f)])\n",
    "            present = str(elongation + ' '+ actions_holistic[np.argmax(res_hol)])\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(present), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "      \n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break       \n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Action Detection Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
